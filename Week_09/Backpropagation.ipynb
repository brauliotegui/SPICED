{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, you need to implement the Feed-Forward-Network first.\n",
    "\n",
    "Below you find a step-by-step tutorial for how to build the backpropagation algorithm from scratch, based on the simple, 2-layer, feed- forward network that you have already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 1\n",
    "\n",
    "Let’s start with a toy data set. The “make moons” data set in sklearn is a nice example for this exercise, since it’s simple and also resembles the non-linear “XOR” problem.\n",
    "\n",
    "Create some training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=50, noise=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 2\n",
    "\n",
    "Make a scatterplot of the 2 columns of X data.\n",
    "Color the data points according to the labels (y).\n",
    "Hint The plt.scatter() function does a good job plotting NumPy arrays.\n",
    "The data should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fd31dbb5550>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 3\n",
    "\n",
    "Since we are modeling our data set with a bias (all ones), we need to add a third column to our X array.\n",
    "\n",
    "Use NumPy to “horizontally stack” an extra column for the bias onto your input:\n",
    "\n",
    "X = np.hstack([X, np.ones((X.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually add a column for bias\n",
    "X = np.hstack([X, np.ones((X.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 4\n",
    "\n",
    "In our simple feed-forward network, we’ll be using the sigmoid activation function for all 3 of our neurons. Remember the sigmoid function from way back in Logistic Regression?\n",
    "\n",
    "Write a python function, sigmoid, that accepts a NumPy array as an input, and returns a NumPy array where all of the original values have been transformed by the sigmoid / logistic function (or, as 3Blue1Brown would call it, the “squishification” function).\n",
    "\n",
    "def sigmoid(x):\n",
    "\n",
    "    ...\n",
    "\n",
    "\n",
    "You’ll know if you wrote it correctly if the following code doesn’t throw an assertion error:\n",
    "\n",
    "\n",
    "a = np.array([-10.0, -1.0, 0.0, 1.0, 10.0])\n",
    "\n",
    "expected = np.array([0.0, 0.27, 0.5, 0.73, 1.0])\n",
    "\n",
    "assert np.all(sigmoid(a).round(2) == expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's model our neurons/ activation functions as a sigmoid function\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''logistic function\n",
    "        takes in a single number, and squishes it\n",
    "        between 0 and 1.'''\n",
    "    \n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([-10.0, -1.0, 0.0, 1.0, 10.0])\n",
    "expected = np.array([0.0, 0.27, 0.5, 0.73, 1.0])\n",
    "\n",
    "assert np.all(sigmoid(a).round(2) == expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 5\n",
    "\n",
    "Refer again to the image of network we’re trying to build. What about all the neural connections / arrows? Those are our weights.\n",
    "\n",
    "These weights should also be represented by NumPy arrays, since they will multiplied (i.e. dot product) by input values before getting passed into an activation function.\n",
    "\n",
    "Initialize the network with random weights (for both the hidden and output layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint (questions to help guide you):\n",
    "- In our simple network, how many weights should we have, in total?\n",
    "- How many weights are in the hidden layer, and how many in the output layer?\n",
    "- If we represent the weights for each layer with a separate numpy array, what should its shape be, given that – for example – the first layer has 3 inputs that feed 2 neurons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the values of the arrows == weights/ parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_weights = np.random.rand(3, 2)    # 3 inputs feeding into 2 neurons\n",
    "outer_weights = np.random.rand(3, 1)     # 3 inputs feeding into 1 neuron   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 6\n",
    "\n",
    "Connect all the pieces and “build” the network!\n",
    "\n",
    "Wrap everything into a python function that “runs” the feed-forward network from beginning to end one time. This function should accept the input data as an argument, as well as the initial random weights.\n",
    "\n",
    "In the annotated template code below, the weights are assumed to be a single argument (e.g. a list of a NumPy arrays), but this of course could be split into separate arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [hidden_weights, outer_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.20971176, 0.17640142],\n",
       "        [0.40567984, 0.06611907],\n",
       "        [0.39220322, 0.86923516]]),\n",
       " array([[0.71571162],\n",
       "        [0.80370787],\n",
       "        [0.7011034 ]])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X, weights):\n",
    "\n",
    "    \"\"\"\n",
    "    1. Calculate the dot product of X\n",
    "       and the weights of the first layer.\n",
    "\n",
    "    2. Apply the sigmoid function on the result.\n",
    "\n",
    "    3. Append an extra column of ones to the result (i.e. the bias).\n",
    "\n",
    "    4. Calculate the dot product of the previous step\n",
    "       with the weights of the second (i.e. outer) layer.\n",
    "\n",
    "    5. Apply the sigmoid function on the result.\n",
    "\n",
    "    6. Return all intermediate results (i.e. anything that is outputted\n",
    "       by an activation function).\n",
    "    \"\"\"\n",
    "    \n",
    "    #1\n",
    "    step1 = np.dot(X, weights[0])\n",
    "    \n",
    "    #2\n",
    "    step2 = sigmoid(step1)\n",
    "    \n",
    "    #3\n",
    "    step3 = np.hstack([step2, np.ones((step2.shape[0], 1))])\n",
    "    \n",
    "    #4 \n",
    "    step4 = np.dot(step3, weights[1])\n",
    "    \n",
    "    #5\n",
    "    step5 = sigmoid(step4)\n",
    "    \n",
    "    #6\n",
    "\n",
    "    return step2, step5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2: Training the Neural Network\n",
    "\n",
    "- Using an algorithm like Gradient Descent\n",
    "- General idea is to slowly (in increments) tweak the parameters such that the prediction is better than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Debugging Check\n",
    "\n",
    "Run your code for the Feed-Forward-Network and make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1, out2 = feed_forward(X, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Write a Loss Function\n",
    "\n",
    "The first thing we’ll need is a loss function. In the gradient descent lesson earlier in the course, we used the mean-squared-error (MSE) as our loss function, which we wanted to minimize. In this case, however, because we are doing a classification problem with a sigmoid activation function as the final layer, we won’t use MSE, but rather the log-loss. The log-loss quantifies how far away are we from the real labels / correct result.\n",
    "\n",
    "According to sklearn: Log-loss is the “function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of the true labels given a probabilistic classifier’s predictions.”\n",
    "\n",
    "Here’s the formula for log-loss. The first task is to transcribe this formula into a Python function.\n",
    "\n",
    "loss=−(ytrue∗log(ypred)+(1−ytrue)∗log(1−ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(ytrue, ypred):\n",
    "    \n",
    "    ''' return the log loss\n",
    "        '''\n",
    "    loss = -(ytrue * np.log(ypred) + (1-ytrue) * np.log(1-ypred))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([0.0, 0.0, 1.0, 1.0])\n",
    "y_pred = np.array([0.01, 0.99, 0.01, 0.99])\n",
    "expected = np.array([0.01, 4.61, 4.61, 0.01])\n",
    "\n",
    "assert np.all(log_loss(y_true, y_pred).round(2) == expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Get Initial Loss\n",
    "\n",
    "Run your feed-forward function, and try getting an array of log-loss values for each of your data points, compared to the actual labels. Note that you’ll need to reshape the ytrue values from a (N, ) array to a (N, 1) array (e.g. both the prediction and the actual values should be (50,1)). Make sure the shape of the loss values is also (N,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1, out2 = feed_forward(X, weights)\n",
    "ytrue = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrue.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_loss = log_loss(ytrue, out2) \n",
    "init_loss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Write a Backpropagation Function\n",
    "\n",
    "Fill in the blanks of the following function, which transcribes the equations from earlier (equations A - E) to run one iteration of the backpropagation algorithm. It takes in a handful of arguments:\n",
    "\n",
    "* the initial weights,\n",
    "* the outputs from the feed-forward process (i.e. both the hidden output and the final output),\n",
    "* the true labels,\n",
    "* the input data,\n",
    "* and the learning rates (we’ll have a separate learning rate for each layer of the network).\n",
    "\n",
    "The function (representing a single iteration of the backpropagation algorithm), should return the modified hidden weights and the modified outer weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(weights,\n",
    "             output1,\n",
    "             output2,\n",
    "             ytrue,\n",
    "             X_input,\n",
    "             LR_O,\n",
    "             LR_H):\n",
    "    \n",
    "    #separate learning rates for outer and inner weights.\n",
    "    wH = weights[0]\n",
    "    wO = weights[1]\n",
    "\n",
    "    '''EQUATION A:'''\n",
    "    ytrue = ytrue.reshape(-1, 1)\n",
    "    error = (output2 - ytrue) * log_loss(ytrue ,output2)\n",
    "\n",
    "    '''EQUATION B:'''\n",
    "    sig_deriv = output2 * (1 - output2)\n",
    "    #derivative of the sigmoid function with respect to the\n",
    "    #hidden output * weights\n",
    "    y_grad = sig_deriv * error\n",
    "\n",
    "    '''EQUATION C:'''\n",
    "    hidden_out_with_bias = np.hstack([output1,np.ones((output1.shape[0],1))])\n",
    "    #don't forget the bias!\n",
    "    delta_wo = np.dot(-y_grad.transpose(), hidden_out_with_bias) * LR_O\n",
    "\n",
    "    #and finally, old weights + delta weights -> new weights!\n",
    "    wO_new = wO + delta_wo.transpose()\n",
    "\n",
    "    '''EQUATION D:'''\n",
    "    sig_deriv_2 = output1 * (1 - output1)\n",
    "    H_grad = sig_deriv_2  * np.dot(y_grad, wO_new[:2].transpose())\n",
    "    #exclude the bias (3rd column) of the outer weights,\n",
    "    #since it is not backpropagated!\n",
    "\n",
    "    '''EQUATION E:'''\n",
    "    delta_wH = np.dot(-H_grad.transpose(), X_input) * LR_H\n",
    "    wH_new = wH + delta_wH.transpose()\n",
    "    #old weights + delta weights -> new weights!\n",
    "\n",
    "    return wH_new, wO_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.06655616, -0.12067024],\n",
       "        [ 2.04393659,  1.74088252],\n",
       "        [ 3.30815992,  3.76452921]]),\n",
       " array([[-2.56083899],\n",
       "        [-2.78669679],\n",
       "        [-4.35917266]]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backprop(weights, out1, out2, y, X, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "wH_new, wO_new = backprop(weights, out1, out2, y, X, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = [wH_new, wO_new]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Run the Backpropagation Algorithm\n",
    "\n",
    "Run your backpropagation algorithm in a loop! Inside the loop:\n",
    "\n",
    "Run your feed-forward function with the X data and the starting weights (which are initially random!).\n",
    "Collect the total sum of the log-loss values into a list, so we can track them over time.\n",
    "Run your backprop function to get the modified weights.\n",
    "At the end of the loop, make your modified weights the new weights for the next cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_VEC = []\n",
    "for i in range(500):\n",
    "    out1, out2 = feed_forward(X, weights)\n",
    "    LOSS_VEC.append(sum(log_loss(y, out2))[0])\n",
    "    new_weights = backprop(weights, out1, out2, y, X, 1.0, 1.0)\n",
    "    weights = new_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Plot Results\n",
    "\n",
    "Use matplotlib to plot the loss of your network over each iteration (i.e. epoch).\n",
    "\n",
    "BONUS: Write a function that calculates the accuracy of your model (hint: you’ll have to round your probabilities up/down to 1/0). Include this function in your loop so that you can also plot accuracy over each epoch. How high can you get the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
